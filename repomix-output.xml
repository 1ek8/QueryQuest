This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
cli/
  lib/
    keyword_search.py
    search_utils.py
  keyword_search_cli.py
.python-version
.repomixignore
pyproject.toml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".python-version">
3.14
</file>

<file path=".repomixignore">
data
.venv
.gitignore
</file>

<file path="pyproject.toml">
[project]
name = "queryquest"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.14"
dependencies = [
    "nltk>=3.9.2",
]
</file>

<file path="cli/lib/search_utils.py">
import json
from pathlib import Path

BM25_K1 = 1.5
BM25_B = 0.75

PROJECT_ROOT = Path(__file__).resolve().parents[2]
MOVIES_PATH = PROJECT_ROOT/'data'/'movies.json'
STOPWORDS_PATH = PROJECT_ROOT/'data'/'stopwords.txt'
CACHE_PATH = PROJECT_ROOT/'cache'

def load_movies() -> list[dict]:
    with open(MOVIES_PATH, "r") as f:
        data = json.load(f)
    return data['movies']

def read_stopwords () -> list[str]:
    with open(STOPWORDS_PATH, "r") as f:
        stopwords = f.read().splitlines()

    return stopwords
</file>

<file path="cli/keyword_search_cli.py">
#!/usr/bin/env python3

import argparse
import json
from lib.search_utils import BM25_B, BM25_K1
from lib.keyword_search import search_command, InvertedIndex

def main() -> None:
    parser = argparse.ArgumentParser(description="Keyword Search CLI")
    subparsers = parser.add_subparsers(dest="command", help="commands")

    search_parser = subparsers.add_parser("search", help="Search movies ")
    search_parser.add_argument("query", type=str, help="Search query")

    search_parser = subparsers.add_parser("build", help="Build Inverse Index ")
    # search_parser.add_argument("query", type=str, help="Search query")

    search_parser = subparsers.add_parser("tf", help="get term frequency ")
    search_parser.add_argument("doc_id", type = int, help="search document")    
    search_parser.add_argument("term", type = str, help="search term")

    search_parser = subparsers.add_parser("idf", help="get inverse document frequency ")
    search_parser.add_argument("term", type = str, help="search term for idf")    

    search_parser = subparsers.add_parser("tfidf", help="get term frequency ")
    search_parser.add_argument("doc_id", type = int, help="search document")    
    search_parser.add_argument("term", type = str, help="search term")

    search_parser = subparsers.add_parser("bm25idf", help="get inverse document frequency for bm25")
    search_parser.add_argument("term", type = str, help="search term for idf")   

    search_parser = subparsers.add_parser("bm25tf", help="get bm25 saturated term frequency ")
    search_parser.add_argument("doc_id", type = int, help="search document")    
    search_parser.add_argument("term", type = str, help="search term") 
    search_parser.add_argument("k1", type = float, help="k1 value", nargs='?', default=BM25_K1) 
    search_parser.add_argument("b", type=float, nargs='?', default=BM25_B, help="Tunable BM25 b parameter")

    args = parser.parse_args()

    match args.command:
        case "search":
            print(f"Searching for: {args.query}")
            results = search_command(args.query)
            for i, movie in enumerate(results):
                print(f"{i}: {movie["title"]}")

        case "build":
            index = InvertedIndex()
            index.build()
            index.save()
        
        case "tf":
            index = InvertedIndex()
            index.load()
            tf = index.get_tf(args.doc_id, args.term)
            print(f"{tf}")

        case "idf":
            index = InvertedIndex()
            index.load()
            idf = index.get_idf(args.term)
            print(f"{idf:.2f}")

        case "tfidf":
            index = InvertedIndex()
            index.load()
            tf = index.get_tf(args.doc_id, args.term)
            idf = index.get_idf(args.term)
            print(f"{idf*tf:.2f}")

        case "bm25idf":
            index = InvertedIndex()
            index.load()
            bm25idf = index.get_bm25_idf(args.term)
            print(f"{bm25idf:.2f}")

        case "bm25tf":
            index = InvertedIndex()
            index.load()
            bm25_tf = index.get_bm25_tf(args.doc_id, args.term, args.k1)
            print(f"{bm25_tf:.2f}")

        case _:
            parser.print_help()


if __name__ == "__main__":
    main()
</file>

<file path="cli/lib/keyword_search.py">
from collections import Counter
from pydoc import doc
from .search_utils import BM25_B, BM25_K1, load_movies
from .search_utils import read_stopwords
from .search_utils import CACHE_PATH
from nltk.stem import PorterStemmer
from pickle import dump, load
import math
import os

import string

def search_command(query: str, top_results: int | None = 5):
    index = InvertedIndex()
    try:
        index.load()
    except FileNotFoundError:
        return []
    
    tokens = preprocess(query)

    seen: set[int] = set()
    results: list[dict] = []
    
    for token in tokens:
        docs = index.get_documents(token)
        for doc in docs:
            if doc in seen:
                continue
            seen.add(doc)
            results.append(index.docmap[doc])
            if(len(results) >= top_results):
                return results
    
    return results

class InvertedIndex:
    def __init__(self):
        self.index: dict[str, set[int]] = {}
        self.docmap: dict[int, dict] = {}
        self.term_frequencies: dict[int, Counter] = {}
        self.doc_lengths: Counter[int, int] = {}

    def get_bm25_tf(self, doc_id, term, k1=BM25_K1):
        b = BM25_B
        if self.__get_avg_doc_length == 0:
            return 0.0
        l_norm = 1 - b + b * (self.doc_lengths[doc_id] / self.__get_avg_doc_length)
        tf = self.get_tf(doc_id, term)
        bm25_tf = (tf * (k1 + 1)) / (tf + k1 * l_norm)
        return bm25_tf

    def __add_document(self, doc_id: int, text: str):
        tokens = preprocess(text)
        if doc_id not in self.term_frequencies:
            self.term_frequencies[doc_id] = Counter()
        for token in tokens:
            if token not in self.index:
                self.index[token] = set()
            self.index[token].add(doc_id)
            self.term_frequencies[doc_id][token] += 1
            self.doc_lengths[doc_id] += 1

    def get_documents(self, term: str) -> list[int]:
        term = cleanse(term)
        return sorted(self.index.get(term, set()))
    
    def build(self):
        movies = load_movies()
        for movie in movies:
            doc_id = movie["id"]
            self.docmap[doc_id] = movie
            movie_input = f"{movie['title']} {movie['description']}"
            self.__add_document(doc_id, movie_input)

    def save(self):
        CACHE_PATH.mkdir(exist_ok=True)
        with open(CACHE_PATH/'index.pkl', 'wb') as f:
            dump(self.index, f)
        with open(CACHE_PATH/'docmap.pkl', 'wb') as f:
            dump(self.docmap, f)
        with open(CACHE_PATH/'term_frequencies.pkl', 'wb') as f:
            dump(self.term_frequencies, f)
        with open(CACHE_PATH/'doc_lengths.pkl', 'wb') as f:
            dump(self.doc_lengths, f)

    def load(self):
        INDEX_PATH = CACHE_PATH/'index.pkl'
        DOCMAP_PATH = CACHE_PATH/'docmap.pkl'
        TF_PATH = CACHE_PATH/'term_frequencies.pkl'
        DOCLENGTH_PATH = CACHE_PATH/'doc_lengths.pkl'

        if not INDEX_PATH.exists() or not DOCMAP_PATH.exists() or not TF_PATH.exists():
            raise FileNotFoundError("Index, docmap or TF file not found in cache directory")
        
        with open(INDEX_PATH, "rb") as f:
            self.index = load(f)
        with open(DOCMAP_PATH, "rb") as f:
            self.docmap = load(f)
        with open(TF_PATH, "rb") as f:
            self.term_frequencies = load(f)
        with open(DOCLENGTH_PATH, "rb") as f:
            self.doc_lengths = load(f)

    def get_tf(self, doc_id, term):
        tokens = preprocess(term)
        if len(tokens) == 0:
            return 0
        if len(tokens) > 1:
            raise ValueError("Expected a single token term for TF calculation")
        term = tokens[0]
        count_dict = self.term_frequencies.get(doc_id)
        if not count_dict:
            return 0
        return int(count_dict.get(term, 0))
    
    def __get_avg_doc_length(self) -> float:
        if len(self.doc_lengths == 0):
            return 0.0
        return sum(self.doc_lengths.values())/len(self.doc_lengths)

    def get_idf(self, term: str):
        total_doc_count = len(self.docmap)
        tokens = preprocess(term)

        if(len(tokens) != 1):
            return 0
        
        token = tokens[0]
        term_match_doc_count = len(self.index.get(token, set()))
        
        return math.log( (total_doc_count + 1)/(term_match_doc_count + 1) )
    
    def get_bm25_idf(self, term: str) -> float:
        tokens = preprocess(term)
        if(len(tokens) != 1):
            return 0
        token = tokens[0]

        N = len(self.docmap)
        df = len(self.index.get(token, set()))

        return math.log((N - df + 0.5) / (df + 0.5) + 1)

def preprocess(input: str) -> list[str]:
    input = cleanse(input)
    input = tokenize(input)
    input = stem(input)
    input = filter(input)
    return input

def cleanse (input: str) -> str:
    input = input.lower()
    result = input.translate(str.maketrans("", "", string.punctuation))
    return result

def tokenize (input : str) -> list[str]:
    tokens = input.split()
    tokens = [token for token in tokens if token]
    return tokens

def stem (input_tokens: list[str]) -> list[str]:
    stemmer = PorterStemmer()
    return [stemmer.stem(token) for token in input_tokens]

def filter(input_token: list[str]) -> list[str]:
    return [token for token in input_token if token not in read_stopwords()]

def token_match (input_tokens: list[str], output_tokens: list[str] ) -> list[str]:
    output_set = set(output_tokens)
    for token in input_tokens:
        # for output_token in  output_tokens:
        if token in output_set:
            return True
    return False
</file>

</files>
