This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
cli/
  lib/
    keyword_search.py
    search_utils.py
  keyword_search_cli.py
.python-version
.repomixignore
pyproject.toml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="cli/lib/search_utils.py">
import json
from pathlib import Path

PROJECT_ROOT = Path(__file__).resolve().parents[2]
MOVIES_PATH = PROJECT_ROOT/'data'/'movies.json'
STOPWORDS_PATH = PROJECT_ROOT/'data'/'stopwords.txt'

def load_movies() -> list[dict]:
    with open(MOVIES_PATH, "r") as f:
        data = json.load(f)
    return data['movies']

def read_stopwords () -> list[str]:
    with open(STOPWORDS_PATH, "r") as f:
        stopwords = f.read().splitlines()

    return stopwords
</file>

<file path=".python-version">
3.14
</file>

<file path=".repomixignore">
data
.venv
.gitignore
</file>

<file path="pyproject.toml">
[project]
name = "queryquest"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.14"
dependencies = [
    "nltk>=3.9.2",
]
</file>

<file path="cli/keyword_search_cli.py">
#!/usr/bin/env python3

import argparse
import json
from lib.keyword_search import search_command

def main() -> None:
    parser = argparse.ArgumentParser(description="Keyword Search CLI")
    subparsers = parser.add_subparsers(dest="command", help="commands")

    search_parser = subparsers.add_parser("search", help="Search movies ")
    search_parser.add_argument("query", type=str, help="Search query")

    args = parser.parse_args()

    match args.command:
        case "search":
            print(f"Searching for: {args.query}")
            results = search_command(args.query)
            for i, movie in enumerate(results):
                print(f"{i}: {movie["title"]}")
            pass
        case _:
            parser.print_help()


if __name__ == "__main__":
    main()
</file>

<file path="cli/lib/keyword_search.py">
from .search_utils import load_movies
from .search_utils import read_stopwords
from nltk.stem import PorterStemmer

import string

def search_command(query: str, top_results: int = None):
    movies = load_movies()
    result = []
    for movie in movies:
        if token_match( preprocess(query), preprocess(movie["title"])):
            result.append(movie)
        if len(result) == top_results:
            break
    return result

def preprocess(input: str) -> str:
    input = cleanse(input)
    input = tokenize(input)
    input = stem(input)
    input = filter(input)
    return input

def cleanse (input: str) -> str:
    input = input.lower()
    result = input.translate(str.maketrans("", "", string.punctuation))
    return result

def tokenize (input : str) -> list[str]:
    result = cleanse(input)
    tokens = result.split()
    tokens = [token for token in tokens if token]
    return tokens

def stem (input_tokens: list[str]) -> list[str]:
    stemmer = PorterStemmer()
    return [stemmer.stem(token) for token in input_tokens]

def filter(input_token: list[str]) -> list[str]:
    return [token for token in input_token if token not in read_stopwords()]

def token_match (input_tokens: list[str], output_tokens: list[str] ) -> list[str]:
    output_set = set(output_tokens)
    for token in input_tokens:
        if token in output_set:
            return True
    return False
</file>

</files>
